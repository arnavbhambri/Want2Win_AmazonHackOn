# -*- coding: utf-8 -*-
"""UserScoring.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bqHHE_DhFQ_AFvzIbiI7i5x2JEc1K9f_
"""

import pandas as pd

# Load the data
user_data = pd.read_csv('/content/user_database_with_legitimacy_score.csv')

# Min-Max normalization function
def normalize(column):
    return (column - column.min()) / (column.max() - column.min())

# Apply normalization to the Legitimacy_Score column
user_data['Normalized_Legitimacy_Score'] = normalize(user_data['Legitimacy_Score'])

# Display the updated dataframe with normalized scores
print(user_data[['User_ID', 'Legitimacy_Score', 'Normalized_Legitimacy_Score']].head())

# Save the updated dataframe to a new CSV file
user_data.to_csv('/content/user_database_normalized.csv', index=False)

import pandas as pd
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Load the data
file_path = '/content/user_database_normalized.csv'
df = pd.read_csv(file_path)

# Feature Engineering
df['return_success_rate'] = df['Successful_Returns'] / df['Return_Attempts']
df['complaint_rate'] = df['Complaints_Raised'] / df['Total_Products_Ordered']
df['complaint_refutation_rate'] = df['Complaints_Refuted'] / df['Complaints_Raised']
df['review_rate'] = df['Total_Reviews_Written'] / df['Total_Products_Ordered']

# Replace infinite values with NaN
df.replace([np.inf, -np.inf], np.nan, inplace=True)

# Fill NaN values
df.fillna(0, inplace=True)

# Define the features and the target variable
features = ['Account_Age', 'Email_Verified', 'Phone_Verified', 'Total_Products_Ordered',
            'Return_Attempts', 'Successful_Returns', 'Complaints_Raised', 'Complaints_Refuted',
            'return_success_rate', 'complaint_rate', 'complaint_refutation_rate', 'review_rate']
target = 'Legitimacy_Score'

# Normalize the features
scaler = MinMaxScaler()
normalized_features = scaler.fit_transform(df[features])
normalized_df = pd.DataFrame(normalized_features, columns=features)

# Define the models
models = {
    'RandomForest': RandomForestRegressor(),
    'GradientBoosting': GradientBoostingRegressor(),
    'XGBoost': XGBRegressor()
}

# Evaluate the models using cross-validation
scores = {name: cross_val_score(model, normalized_df, df[target], cv=5).mean() for name, model in models.items()}

# Select the best model
best_model_name = max(scores, key=scores.get)
best_model = models[best_model_name]

# Train the best model on the full dataset
best_model.fit(normalized_df, df[target])

# Extract feature importances (weights)
weights = dict(zip(features, best_model.feature_importances_))

# Display the best model and the weights
best_model_name, weights