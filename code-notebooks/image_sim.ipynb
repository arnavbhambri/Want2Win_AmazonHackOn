{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('product_database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, GlobalAveragePooling1D\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'product_database.csv'\n",
    "product_data = pd.read_csv(file_path)\n",
    "\n",
    "# Combine 'Product_Name' and 'About_Product' columns for text input\n",
    "product_data['combined_text'] = product_data['Product_Name'] + \" \" + product_data['About_Product']\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to the combined text\n",
    "product_data['cleaned_text'] = product_data['combined_text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    wayona nylon braided usb to lightning fast cha...\n",
      "1    ambrane unbreakable w  a fast charging m braid...\n",
      "2    sounce fast phone charging cable  data sync us...\n",
      "3    boat deuce usb   in  typec  micro usb stress r...\n",
      "4    portronics konnect l m fast charging a  pin us...\n",
      "Name: cleaned_text, dtype: object\n",
      "[[851, 187, 137, 11, 3, 278, 23, 18, 1, 41, 74, 13, 30, 5, 78, 28, 64, 142, 56, 108, 555, 354, 7, 295, 53, 220, 30, 4, 78, 2522, 78, 1134, 1134, 896, 1134, 1134, 2742, 2743, 142, 64, 108, 677, 2744, 97, 1, 68, 252, 2341, 1643, 74, 15, 17, 42, 1, 74, 678, 45, 6, 530, 46, 30, 4, 48, 18, 1102, 1201, 18, 1382, 16, 26, 2037, 100, 187, 137, 34, 4, 165, 512, 556, 1, 1898, 187, 814, 2038, 1279, 468, 2, 128, 1899, 15, 480, 370, 1, 1325, 6, 751, 3, 124, 2039, 828, 385, 15, 9, 156, 3, 611, 406, 8, 83, 19, 1383, 1384, 1900, 304, 1806, 951, 1807, 589, 1503, 2523, 313, 24, 1, 481, 225, 1202, 212, 2, 2040, 2041, 7, 8, 211, 246, 12, 1135, 48, 1901, 16, 1003, 106, 626, 1236, 3, 269, 494], [852, 737, 37, 6, 23, 18, 82, 137, 25, 22, 13, 5, 267, 316, 371, 61, 25, 22, 29, 412, 87, 130, 41, 74, 158, 42, 4026, 27, 30, 4, 33, 25, 22, 372, 29, 31, 15, 63, 93, 306, 338, 181, 1504, 1103, 1441, 482, 357, 293, 65, 389, 873, 2524, 158, 18, 737, 131, 7, 531, 137, 656, 4, 1004, 1721, 2042, 15, 9, 1722, 13, 32, 815, 31, 1902, 43, 513, 1442, 2043, 172, 15, 59, 244, 172, 7, 319, 51, 9, 2186, 829, 1036, 161, 8, 1808, 193, 13, 16, 829, 109, 161, 6, 319, 1809, 373, 6, 23, 18, 1, 130, 41, 152, 46, 313, 257, 24, 19, 2, 157, 7, 211], [1810, 23, 98, 18, 13, 41, 74, 11, 13, 30, 5, 78, 28, 64, 142, 56, 108, 703, 29, 23, 76, 41, 4027, 167, 203, 4028, 1, 3032, 247, 1104, 3443, 373, 469, 73, 1, 853, 1, 1326, 18, 41, 152, 46, 4, 21, 3, 590, 1723, 46, 5007, 4, 78, 28, 64, 142, 56, 108, 703, 29, 390, 3444, 1237, 1, 4029, 288, 131, 7, 1005, 1, 165, 247, 67, 307, 3, 3033, 830, 1, 4030, 138, 53, 73, 1136, 3, 2, 3034, 768, 2, 3035, 34, 17, 3036, 45, 1811, 830, 1, 3037, 1203, 5, 341, 107, 1, 370, 1037, 391, 512, 288, 1, 2525, 1443, 2526, 87, 51, 3, 221, 2, 232, 1160, 815, 1505, 1, 99, 262, 6, 3038, 382, 51, 663, 347, 205, 4, 6, 589, 117, 14, 1, 57, 723, 1280, 382, 514, 417, 1644, 3039, 481, 1, 508, 225, 201, 57, 2745, 3, 12, 355, 183, 12, 17, 211, 4, 3040, 236, 3041, 738, 6, 439, 1645, 201, 3, 2746, 2, 2747, 279, 125, 14, 73], [320, 2342, 11, 10, 94, 145, 11, 612, 307, 1038, 390, 13, 4, 6, 23, 18, 130, 41, 418, 831, 692, 1, 679, 82, 4031, 693, 2, 320, 2342, 11, 10, 13, 9, 30, 4, 267, 316, 228, 874, 66, 386, 26, 917, 1, 33, 61, 29, 4, 94, 36, 347, 36, 145, 11, 2527, 212, 6, 23, 18, 1, 41, 1812, 4, 530, 74, 45, 1903, 165, 187, 137, 986, 118, 15, 390, 1, 1646, 428, 566, 2528, 664, 1006, 1161, 556, 118, 15, 897, 358, 4, 831, 692, 4, 679, 1071, 107, 5, 1162, 3445, 2529, 1, 321, 34, 314, 6, 680, 95, 166, 3446, 11, 13, 314, 6, 162, 319, 10, 172, 5, 330, 918, 308, 166, 184, 24, 19, 2, 157, 7, 211], [795, 1039, 213, 82, 23, 18, 6, 410, 11, 13, 4, 42, 74, 185, 5, 78, 64, 295, 42, 74, 185, 20, 13, 77, 4, 18, 41, 74, 2748, 73, 89, 1005, 187, 89, 3, 126, 796, 32, 2, 133, 7, 2, 13, 9, 769, 2044, 128, 2, 13, 9, 341, 694, 193, 109, 952, 5, 63, 44, 39, 5, 8, 274, 45, 102, 16, 2045, 2046, 13, 9, 613, 1724, 7, 34, 1, 1506, 1, 9, 1385, 3, 241, 2, 206, 274, 1, 188, 204, 2343, 220, 30, 4, 33, 29, 161, 78, 797, 28, 1204, 72, 72, 64, 56, 64, 108, 1, 64, 142]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(product_data['cleaned_text'])\n",
    "sequences = tokenizer.texts_to_sequences(product_data['cleaned_text'])\n",
    "\n",
    "\n",
    "print(product_data['cleaned_text'].head())\n",
    "print(sequences[:5])\n",
    "# Pad the sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "37/37 [==============================] - 1s 14ms/step - loss: 7.2987 - accuracy: 8.5324e-04 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 7.2814 - accuracy: 8.5324e-04 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 7.2666 - accuracy: 0.0017 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 7.2372 - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 7.1914 - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(seq) for seq in sequences])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Prepare labels (we can use Product_ID as dummy labels for now)\n",
    "labels = product_data['Product_ID']\n",
    "\n",
    "# Define the model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(len(product_data['Product_ID'].unique()), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(padded_sequences, labels, epochs=10, batch_size=32, validation_split=0.2)\n",
    "# Save the model\n",
    "model.save('product_similarity_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
